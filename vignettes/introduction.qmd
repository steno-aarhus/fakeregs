---
title: "Introduction to Parquet, Arrow and DuckDB in R"
author: "Anders Aasted Isaksen"
format: html
editor: visual
---

# Introduction

Modern, efficient data processing tools can help you reduce the amount of memory you need to process your data. In addition, they can dramatically reduce the time it takes to run your processing. This document contains a short introductory description on what Parquet, Arrow and DuckDB are, and how/when to use them.

### Memory and data storage formats

Register data is stored on disk, and from there it is read/loaded into the computer's memory (RAM) where it can be processed by the computer's "math brain" (CPU). Data that is saved/stored on disk persists through sessions, whereas memory is cleared at the end of each session (or, on your local computer, when you power it off). So, once any processing is done, the results are usually held in memory, but needs to be stored to disk before you can log off/crash the R session without losing them.

Disk space is usually not a limiting factor, but memory capacity is a hard limit to the size of data that your computer can work with, making it a very important thing to consider in your analyses. On Statistics Denmark's servers, you are working on a virtual machine running on one of their servers, and you will be sharing the server's RAM with all other users running analyses on their virtual machines. When the server's total memory is getting close to full (you can gauge this by the color of the "memory report" icon in RStudio's Environment panel - red color is bad), your analyses will run much, much slower (and risk crashing/failing). So, while the server has a very large amount of total RAM (e.g 1,000 GigaBytes), you (and everyone else on the server) will probably experience problems if one or more users are occupying several hundreds of GBs of RAM at the same time.

So, for everyone's sake, it's a good idea to be mindful of your memory footprint and only take up as little as needed - and, when you need to occupy very large amounts of RAM, only do so for as short a time as needed.

### Processing engines

When processing data in R scripts, you're using R's processing engine (sometimes referred to as the *back-end*) by default. This seems obvious, and chances are that you've never used - or needed, but we'll get back to that - any other engine before. When processing very large amounts of data, e.g. nation-wide registers, there are some advantages to using other processing engines, and clever people have created R packages that translate R and `dplyr` commands to the language (SQL queries) of their respective engines, allowing you to use some of those engines from within your R session, using standard `dplyr` syntax.

In this guide, we'll use two of these engines - Arrow, accessible through the `arrow` package, and DuckDB, accessible through the `duckplyr` package - both of which are installed on the virtual machines of Statistics Denmark and can be used with standard `dplyr` syntax as if they were normal R objects.

#### What are these new things?

-   Parquet
    -   Apache Parquet is an open-source project made by the Apache Foundation. It is **a column-oriented data file format designed for efficient data storage and retrieval**
    -   In plain language, this means that it's a way to **store data** in files on disk in a way that saves disk space and let's the computer **read data** from the disk as fast as possible.
    -   Parquet files can be created with several tools, but in R this is mainly done using functions from the `arrow` package (`write_parquet()` or `write_dataset()`)
    -   Data stored in Parquet files can be read into other statistical packages, such as Python and Stata (upcoming version), so it supports a diverse group of users.
-   Arrow
    -   Apache Arrow is also an open-source project made by the Apache Foundation. It is **a language-independent memory format organized for efficient analytic operations**
    -   In plain language, this means that it's a way to **structure data** in a way that let's the computer **process data** as efficiently as possible. How this is done and what separates Arrow from other formats like R, SAS or DuckDB is beyond the scope of this guide. This is all done "under the hood" anyway, and as a user you don't need to know the inner workings to take advantage of it.
    -   The `arrow` R-package provides R users access to many of the features of Apache Arrow through base R and `tidyverse` functions.
-   DuckDB is an open-source project made by DuckDB Labs/the DuckDB Foundation. It is **a fast, in-process, portable analytical database system**
    -   In plain language, this means that it's a way to **structure and/or store data** in a way that facilitates **efficient data processing and analysis**. The in-process and portable part refers to DuckDB being interoperable with a lot of other data storage formats and programming language processing engines, including Arrow and R, which allows the user to efficiently read/convert most data types into a DuckDB database without the need to set up a dedicated SQL database server in a separate workflow.
    -   In a sense, DuckDB can do alone what Parquet and Arrow can do in combination, but it also plays well with them, as it can e.g. read Parquet files or Arrow Table data very fast.
    -   The `duckplyr` R-package runs all of your existing `dplyr` code, using DuckDB where possible to calculate results faster and allow processing of larger-than-memory datasets (by being very memory-efficient and by sometimes writing data to temporary `.duckdb` files on disk if needed).
-   As you can see, the aims of `duckplyr` and `arrow` are similar: to let R users use a more efficient storage and processing engine than native R, without having to learn the language of those engines. And from an R-user's perspective, they are similar to work with, as they rely mainly `dplyr` functions and use `dplyr::compute()` to move data into the right place for the engine to process and `dplyr::collect()` to force execution of commands and pull the results into R.

#### What are main pros and cons to using these external engines in R:

-   Pros:
    -   They're much more computationally efficient, so you'll get your processing done much faster
    -   They take up less memory resources on the server, so you're less likely to crash your session or experience instability or unresponsiveness (and so are everyone else using the server at the same time as you).
-   Cons:
    -   Objects created by commands interacting with these engines don't look and behave exactly as you're used to when working in R, so you will need to get to know them.
    -   They don't support **all** of R's functionality. Their main purpose is to facilitate data cleaning (e.g. row filtering, variable selection/creation, table joins) and relatively simple analytics (e.g. sorting, calculate means etc.). You will still be doing your regressions in R.

### Getting started with Arrow/`arrow` and DuckDB/`duckplyr`

Remember: The goal of using `arrow` or `duckplyr` is to use their engines to read and wrangle the massive raw datasets that make up most registers. At some point - after a lot of filtering, selection, joining etc. - this data will be cleaned to the variables you actually use in your analyses, which will fit in a size that R can handle without breaking a sweat, e.g. a handful of tables consisting of million of rows. At this point, there is no longer much advantage to using either back-end over R's engine, and you can safely use R's engine to execute your custom functions or other operations not support by the other back-ends. You may find yourself needing to use functions that aren't supported by `arrow` or `duckplyr` earlier in your pipeline, and that's fine as well - just try to think of ways to reduce the size of the data as much as possible before having to fall back to using R's engine. If need be, you can even export the data back onto the Arrow or DuckDB engine after it's been pulled into R, but that's beyond the scope of this guide.

The main point of surprise, confusions, and frustration with these engines is usually their *lazy evaluation* vs. R's default *eager evaluation*, which leads to their objects and outputs by default being connections or pointers to a not-yet-needed-and-therefore-non-existing table of data, where R would have executed the computations and generated a table that you could see with your own eyes.

A practical way to understand the *lazy evaluation* used by both `arrow` and `duckplyr`, is to think of it like grocery shopping. R's default *eager evaluation* goes shopping as soon as you think of some ingredient (execute a command in R) that you will need for cooking in the coming days, whereas *lazy evaluation* writes the thoughts down in a shopping list (a SQL query), but doesn't go shopping until you actually start cooking, and only buys the ingredients needed for the dish you've started to cook. This way, it only buys the ingredients (reads the data) you actually need, not what you *thought* or *planned* to use, so it doesn't have to carry as much, and nothing goes to waste. Furthermore, since the ingredients stay in the store as long as possible, it doesn't fill up your refrigerator (memory) with unnecessary items (data). This is why *lazy evaluation* is sometimes called *just-in-time evaluation*.

### Arrow/`arrow` and DuckDB/`duckplyr` in practice

Both packages work by "hi-jacking" `dplyr` functions called on `arrow` or `duckplyr` table objects and translating them to their engines. So, once you've loaded your data as such an object, you simply write the same `dplyr` code you're used to writing (and if you use `dplyr` commands on a "normal" R object, e.g. a `tibble`, this is handled by `dplyr` as if `arrow` and `duckplyr` didn't exist).

Both packages use `dplyr::collect()` to force execution of commands in their engines and pull the results into R, and `dplyr::compute()` to connect data from another source to their engine (e.g. move/convert a `tibble` in R to an Arrow or DuckDB table). `arrow` and `duckplyr` support each other very well, and you can efficiently convert an Arrow Table/dataset into a DuckDB database (and vice versa). The examples below will make use of this when loading in the data - which is stored in Parquet files as a dataset of Arrow Tables - and convert the Arrow connection to a DuckDB connection.

A final note on joining tables before proceeding: To execute joins on their engines, both packages require both of the join's incoming tables to formatted as either two Arrow or DuckDB tables. Furthermore, the tables must have the same connection source, but this is beyond the scope of this guide (for now at least)

### `arrow` vs. `duckplyr`

In both `arrow` and `duckplyr`, you can use `dplyr::collect()` to signal their engine to start executing the commands (you start cooking), but other commands can also force them to execute any "shopping list" of commands previously executed on their objects in R.

While `arrow` supports a lot of operations from R, it will sometimes throw an error when encountering a few operators. On the contrary,`duckplyr` supports nearly all`dplyr`commands, and when`duckplyr` encounters an R command it cannot translate to something the DuckDB engine can execute, it will - in most cases - automatically pull the data into R and "fall back" to executing them in R's engine.

For this reason, it's usually worth using DuckDB through `duckplyr` rather than Arrow through Â´arrow\`, but both are blazing fast and can get you through the most heavy parts of your data processing.

One reason to use `arrow` instead of `duckplyr` is if your data is saved to disk as Arrow Tables in Parquet files, and you don't want to spend the first few minutes (and system resources) it takes `dplyr::compute()` to convert a whole register into a DuckDB format that `duckplyr` can work with. By default, the DuckDB database produced by `dplyr::compute()` is temporary and is deleted from memory (and disk, if it exceeds the memory capacity of the system) when the session ends, but you can choose to save the DuckDB database on the disk as a `.duckdb` file that you can read directly from R with `duckplyr` later if you wish to . But for now, this is out of scope of this guide.

### Further reading:

There is good official documentation to get started using `arrow`and `duckplyr`:

-   `arrow`:
    -   [data wrangling](https://arrow.apache.org/docs/r/articles/data_wrangling.html)
    -   [support & limitations](https://arrow.apache.org/docs/r/reference/acero.html)
-   `duckplyr`:
    -   [supported functions](https://duckplyr.tidyverse.org/articles/limits.html)
    -   [unsupported dplyr functions](https://duckplyr.tidyverse.org/reference/unsupported.html)
