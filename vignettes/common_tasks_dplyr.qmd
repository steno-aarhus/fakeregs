---
title: "Common data processing tasks: examples using dplyr"
author: "Anders Aasted Isaksen"
format: html
editor: visual
---

# Introduction

This Quarto document uses synthetic register data to run a data processing pipeline and illustrate common tasks encountered when conducting register-based research.

## Setup chunks

The beginning of most sections of this walkthrough contains a code chunk that sets up each register by generating the synthetic register data and storing it as Parquet files on disk in the `data-parquet` sub-folder of the current directory. The data is read in from there in subsequent code chunks.

In some real-world scenarios, you may be on your own in a project with only the raw SAS data. In this situation, you would first need to convert your raw SAS files to a more practical format like Parquet (the `fastregs` package can help you) and store them in a similar structure as is done in this guide (data from each register is stored inside separate folders). Then you can load your data from those Parquet files in the same way as shown below.

### Let's go

Let's get to work with some actual fake data and real code!

Through-out the examples, I use the `package::function` syntax to be explicit about which packages the functions are from. In a normal workflow, this isn't necessary after loading the libraries - e.g. after running `library(dplyr)`, you can use e.g. `select()` instead of `dplyr::select()` just fine.

First, we need to generate the synthetic data to mimic the DST environment and run the examples on:

```{r setup-dependencies}

# Check if necessary packages are installed:
packages_to_check <- c("dplyr", "purrr", "tibble", "lubridate", "here", "arrow", "duckplyr", "fs")

data.frame(
  Package = packages_to_check,
  IsInstalled = packages_to_check %in% row.names(installed.packages())
)




# Source (run) the script containing the functions to generate synthetic data to import them:
source(here::here("R", "generate_data.R"))
```

## BEF

```{r setup-bef}
# Setup synthetic data: Create BEF directory/folder:
fs::dir_create(here::here("data-parquet", "bef"))

# Generate BEF and save to disk
arrow::write_dataset(
  generate_bef(
    background_df = generate_background_pop()
    ),
  here::here("data-parquet", "bef"),
  format = "parquet",
  partitioning = "year")

```

Tasks:

1.  Load in BEF
    1.  Normally, this will be from disk: either single-year files or a combined dataset containing data of all years.
    2.  Here, we mimic having the raw data converted to Parquet format with files from each register stored in separate folders (similar to the output from the `fastregs` package. So first, we generate the synthetic data, save it to disk as `.parquet` files by year, and then load it from disk.
2.  Filter the data to what is needed for your planned analyses.
    1.  This is a good way to reduce the size of the data you're working with, which will speed processing times up and make your sessions more stable.
    2.  For example, you can filter your data to the range of years and/or ages used in your analyses.
    3.  The list of pnr-numbers that you identify in this step can be used to filter all the other data sources. If your analyses are restricted to a population of individuals with a certain disease, e.g. diabetes or cardiovascular disease, it may be useful to define these variables early in your workflow, and combine them with bef to allow you to filter the data to this study population from the start.
3.  Convert raw codes to usable values.

### 1: Load

The key step here is to read the Arrow Tables from the Parquet files (`bef_folder |> arrow::open_dataset(unify_schemas = TRUE)`) into DuckDB (by piping it into `arrow::to_duckdb() |> dplyr::compute()`). If can add operations betweent the two parts to e.g. filter the data in Arrow before converting it to DuckDB. This way you will have less data to convert to DuckDB, and spend less time and system memory on the conversion.

Depending on the functions you need to execute, and the `arrow` and `duckplyr` package versions, you can sometimes get away with not using `dplyr::compute()` to load the data into DuckDB, but it's a good habit to exercise.

```{r load-bef}

# Load BEF data:

bef_folder <- here::here("data-parquet", "bef")
bef_ddb <- bef_folder |> arrow::open_dataset(unify_schemas = TRUE) |>
  # You can add cleaning steps in Arrow here before converting to DuckDB
  arrow::to_duckdb() |> dplyr::compute()

bef_df <- bef_ddb |> dplyr::collect()

bef_df
```

### 2: Filter

In this example, we only need adults alive and residing in Denmark between 2020 and 2024 for our analyses.

```{r}
bef_filtered <-
  bef |> dplyr::filter(year %in% 2020:2024 & ALDER >= 18) |>
  dplyr::select(PNR, year, FOED_DAG, KOEN, CIVST, REG, OPR_LAND)

# Keep the list of pnr's for later filtering:
filtered_pnrs <- bef_filtered |> dplyr::pull(PNR)
```

### 3: Convert

```{r}

bef_clean <- bef_filtered |>
  dplyr::mutate(
    PNR = PNR,
    year = year,
    do_birth = as.Date(FOED_DAG),
    
    sex = dplyr::if_else(KOEN == '1', 'Male', 'Female'),
    
    marital_status = dplyr::case_when(
      CIVST %in% c("G", "P") ~ "Married",
      CIVST %in% c("F", "O", "E", "L") ~ "Divorced or widowed",
      CIVST == "U" ~ "Unmarried",
      TRUE ~ NA_character_
    ),
    
    region = dplyr::case_when(
      REG == 81 ~ "North Denmark Region",
      REG == 82 ~ "Central Denmark Region",
      REG == 83 ~ "South Denmark Region",
      REG == 84 ~ "Capital Region",
      REG == 85 ~ "Zealand Region",
      TRUE ~ NA_character_
    ),
    
    immigrant = OPR_LAND != 5100
  )

bef_clean |>  dplyr::collect()
```

## LPR

### LPR2

```{r setup-lpr2}

# Create lpr_adm directory/folder:
fs::dir_create(here::here("data-parquet", "lpr_adm"))

# Generate lpr_adm and save to disk
arrow::write_dataset(
  generate_lpr_adm(
    background_df = generate_background_pop()
    ),
  here::here("data-parquet", "lpr_adm"),
  format = "parquet",
  partitioning = "year")

# Create lpr_diag directory/folder:
fs::dir_create(here::here("data-parquet", "lpr_diag"))

# Generate lpr_adm and save to disk (year-partitioning the synthetic data is not feasible here)
arrow::write_dataset(
  generate_lpr_diag(generate_lpr_adm(
    background_df = generate_background_pop()
  )),
  here::here("data-parquet", "lpr_diag"),
  format = "parquet"
)
```

#### lpr_adm & lpr_diag

Let's look at `lpr_adm`:

```{r load-lpr-adm}
# Load lpr_adm data:

lpr_adm_folder <- here::here("data-parquet", "lpr_adm")
lpr_adm_ddb <- lpr_adm_folder |> arrow::open_dataset(unify_schemas = TRUE) |>
  # You can add cleaning steps in Arrow here before converting to DuckDB
  arrow::to_duckdb() |> dplyr::compute()

lpr_adm_df <- lpr_adm_ddb |> dplyr::collect()

head(lpr_adm_df)
```

And `lpr_diag:`

```{r load-lpr-diag}
# Load lpr_diag data:
lpr_diag_folder <- here::here("data-parquet", "lpr_diag")
lpr_diag_ddb <- lpr_diag_folder |> arrow::open_dataset(unify_schemas = TRUE) |>
  # You can add cleaning steps in Arrow here before converting to DuckDB
  arrow::to_duckdb() |> dplyr::compute()

lpr_diag_df <- lpr_diag_ddb |> dplyr::collect()

head(lpr_diag_df)
```

### LPR3

#### lpr_a_kontakt & lpr_a_diagnose

```{r setup-lpr-a}

# Create lpr_a_kontakt directory/folder:
fs::dir_create(here::here("data-parquet", "lpr_a_kontakt"))

# Generate lpr_a_kontakt and save to disk
arrow::write_dataset(
  generate_lpr_a_kontakt(
    background_df = generate_background_pop()
    ),
  here::here("data-parquet", "lpr_a_kontakt"),
  format = "parquet")

# Create lpr_a_diagnose directory/folder:
fs::dir_create(here::here("data-parquet", "lpr_a_diagnose"))

# Generate lpr_a_diagnose and save to disk
arrow::write_dataset(
  generate_lpr_a_diagnose(generate_lpr_a_kontakt(
    background_df = generate_background_pop()
    )),
  here::here("data-parquet", "lpr_a_diagnose"),
  format = "parquet"
)
```
