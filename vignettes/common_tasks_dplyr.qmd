---
title: "Common data processing tasks: examples using dplyr"
author: "Anders Aasted Isaksen"
format: html
editor: visual
---

# Introduction

This Quarto document uses synthetic register data to run a data processing pipeline and illustrate common tasks encountered when conducting register-based research.

## Setup

The synthetic registers are generated during this R session, stored as Parquet files on disk in the `data-parquet` sub-folder using the `arrow` package, and then read in from there.

If you're on your own in a project with only the raw SAS data, you would first need to convert your raw SAS files to a more practical format (the `fastregs` packages can help you), and then read your data in from there. Either in the raw structure as separate files for each year (e.g. `bef202412.csv` or `.rds`), or - much faster and more convenient - as a dataset of `Arrow Tables` stored as `.parquet` using the `arrow` package.

### Let's go

Let's get to work with some actual fake data and real code!

Through-out the examples, I use the `package::function` syntax to be explicit about which packages the functions are from. In a normal workflow, this isn't necessary after loading the libraries - e.g. after running `library(dplyr)`, you can use e.g. `select()` instead of `dplyr::select()` just fine.

First, we need to generate the synthetic data to mimic the DST environment and run the examples on:

```{r setup}
knitr::opts_chunk$set(message = FALSE)
# Load necessary packages:
library(dplyr)
library(purrr)
library(tibble)
library(lubridate)
library(here)
library(arrow)
library(duckplyr)
library(fs)

# Source (run) the script containing the functions to generate synthetic data to import them:
source(here::here("R", "generate_data.R"))

# Create BEF directory/folder:
fs::dir_create(here::here("data-parquet", "bef"))

# Generate BEF and save to disk
arrow::write_dataset(generate_bef(generate_background_pop()), here::here("data-parquet", "bef"), format = "parquet", partitioning = "year")
```

## BEF

Tasks:

1.  Load in BEF
    1.  Normally, this will be from disk: either single-year files or a combined dataset containing data of all years.
    2.  Here, we mimic having the raw data converted to Parquet format with files from each register stored in separate folders (similar to the output from the `fastregs` package. So first, we generate the synthetic data, save it to disk as `.parquet` files by year, and then load it from disk.
2.  Filter the data to what is needed for your planned analyses.
    1.  This is a good way to reduce the size of the data you're working with, which will speed processing times up and make your sessions more stable.
    2.  For example, you can filter your data to the range of years and/or ages used in your analyses.
    3.  The list of pnr-numbers that you identify in this step can be used to filter all the other data sources. If your analyses are restricted to a population of individuals with a certain disease, e.g. diabetes or cardiovascular disease, it may be useful to define these variables early in your workflow, and combine them with bef to allow you to filter the data to this study population from the start.
3.  Convert raw codes to usable values.

### 1: Load

The key step here is to read the Arrow Tables from the Parquet files (`bef_folder |> arrow::open_dataset(unify_schemas = TRUE)`) into DuckDB (by piping it into `arrow::to_duckdb() |> dplyr::compute()`). If can add operations betweent the two parts to e.g. filter the data in Arrow before converting it to DuckDB. This way you will have less data to convert to DuckDB, and spend less time and system memory on the conversion.

Depending on the functions you need to execute, and the `arrow` and `duckplyr` package versions, you can sometimes get away with not using `dplyr::compute()` to load the data into DuckDB, but it's a good habit to exercise.

```{r}
bef_folder <- here::here("data-parquet", "bef")
bef <- bef_folder |> arrow::open_dataset(unify_schemas = TRUE) |>
  # Add cleaning steps in Arrow here
  arrow::to_duckdb() |> dplyr::compute()

bef |> dplyr::collect()
```

### 2: Filter

In this example, we only need adults alive and residing in Denmark between 2020 and 2024 for our analyses.

```{r}
bef_filtered <-
  bef |> dplyr::filter(year %in% 2020:2024 & ALDER >= 18) |>
  dplyr::select(PNR, year, FOED_DAG, KOEN, CIVST, REG, OPR_LAND)

# Keep the list of pnr's for later filtering:
filtered_pnrs <- bef_filtered |> dplyr::pull(PNR)
```

### 3: Convert

```{r}

bef_clean <- bef_filtered |>
  dplyr::mutate(
    PNR = PNR,
    year = year,
    do_birth = as.Date(FOED_DAG),
    
    sex = dplyr::if_else(KOEN == '1', 'Male', 'Female'),
    
    marital_status = dplyr::case_when(
      CIVST %in% c("G", "P") ~ "Married",
      CIVST %in% c("F", "O", "E", "L") ~ "Divorced or widowed",
      CIVST == "U" ~ "Unmarried",
      TRUE ~ NA_character_
    ),
    
    region = dplyr::case_when(
      REG == 81 ~ "North Denmark Region",
      REG == 82 ~ "Central Denmark Region",
      REG == 83 ~ "South Denmark Region",
      REG == 84 ~ "Capital Region",
      REG == 85 ~ "Zealand Region",
      TRUE ~ NA_character_
    ),
    
    immigrant = OPR_LAND != 5100
  )

bef_clean |>  dplyr::collect()
```
