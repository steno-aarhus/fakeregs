---
title: "Common data processing tasks: examples using dplyr"
author: "Anders Aasted Isaksen"
format: html
editor: visual
---

# Introduction

This Quarto document uses synthetic register data to run a data processing pipeline and illustrate common tasks encountered when conducting register-based research.

## Setup chunks

The beginning of most sections of this walkthrough contains a code chunk that sets up each register by generating the synthetic register data and storing it as Parquet files on disk in the `data-parquet` sub-folder of the current directory. The data is read in from there in subsequent code chunks.

In some real-world scenarios, you may be on your own in a project with only the raw SAS data. In this situation, you would first need to convert your raw SAS files to a more practical format like Parquet (the `fastregs` package can help you) and store them in a similar structure as is done in this guide (data from each register is stored inside separate folders). Then you can load your data from those Parquet files in the same way as shown below.

One final note: the variable names in the fake data used here are lower-cased, whereas the real data often has inconsistent casing, which you will need to process according. At the time of writing, the newest version of `duckplyr` (1.1.3) seems to support converting all column names e.g. to lower-case with `dplyr::rename_with()`, e.g. `dplyr::rename_with(bef_ddb, tolower)`.

### Let's go

Let's get to work with some actual fake data and real code!

Through-out the examples, I use the `package::function` syntax to be explicit about which packages the functions are from. In a normal workflow, this isn't necessary after loading the libraries - e.g. after running `library(dplyr)`, you can use e.g. `select()` instead of `dplyr::select()` just fine.

We need to generate the synthetic data to mimic the DST environment and run the examples on. But first, make sure you have the necessary packages installed, if you're running this as a script:

```{r setup-dependencies}

# Check if necessary packages are installed:
packages_to_check <- c("dplyr", "purrr", "tibble", "lubridate", "here", "arrow", "duckplyr", "fs")

data.frame(
  Package = packages_to_check,
  IsInstalled = packages_to_check %in% row.names(installed.packages()),
  Version = sapply(packages_to_check, function(x) {
    tryCatch(as.character(packageVersion(x)), error = function(e) "Not installed")
  })
)


# Source (run) the script containing the functions to generate synthetic data to import them:
source(here::here("R", "generate_data.R"))
```

## BEF

```{r setup-bef}
# Setup synthetic data: Create BEF directory/folder:
fs::dir_create(here::here("data-parquet", "bef"))

# Generate BEF and save to disk
arrow::write_dataset(
  generate_bef(
    background_df = generate_background_pop()
    ),
  here::here("data-parquet", "bef"),
  format = "parquet",
  partitioning = "year")

```

Tasks:

1.  Load in BEF
    1.  Normally, this will be from disk: either single-year files or a combined dataset containing data of all years.
    2.  Here, we mimic having the raw data converted to Parquet format with files from each register stored in separate folders (similar to the output from the `fastregs` package. So first, we generate the synthetic data, save it to disk as `.parquet` files by year, and then load it from disk.
2.  Filter the data to what is needed for your planned analyses.
    1.  This is a good way to reduce the size of the data you're working with, which will speed processing times up and make your sessions more stable.
    2.  For example, you can filter your data to the range of years and/or ages used in your analyses.
    3.  The list of pnr-numbers that you identify in this step can be used to filter all the other data sources. If your analyses are restricted to a population of individuals with a certain disease, e.g. diabetes or cardiovascular disease, it may be useful to define these variables early in your workflow, and combine them with bef to allow you to filter the data to this study population from the start.
3.  Convert raw codes to usable values.

### 1: Load

The key step here is to read the Arrow Tables from the Parquet files (`bef_folder |> arrow::open_dataset(unify_schemas = TRUE)`) into DuckDB (by piping it into `arrow::to_duckdb() |> dplyr::compute()`). You can add operations betweent the two parts to e.g. filter the data in Arrow before converting it to DuckDB. This way you will have less data to convert to DuckDB, and spend less time and system memory on the conversion.

`arrow` and `duckplyr` will often help you out if you forget it, and you can sometimes get away with not calling `dplyr::compute()` to load the data into DuckDB or `dplyr::collect()` to load it from Arrow/DuckDB into R, but it's a good habit to be explicit and conscious of when those commands are called.

Let's look at `bef`. In order to look at it in R, we have to collect it into R. Make sure to called `head()` or other filtering operations before `dplyr::collect()` to avoid loading the entire dataset into R, when you just want a short glance at its contents:

```{r load-bef}

# Load BEF data:

bef_folder <- here::here("data-parquet", "bef")
bef_ddb <- bef_folder |> arrow::open_dataset(unify_schemas = TRUE) |>
  # You can add cleaning steps in Arrow here before converting to DuckDB
  arrow::to_duckdb() |> dplyr::compute()

bef_df <- bef_ddb |> head() |>  dplyr::collect()

bef_df
```

A quick way to tell if your object is an R data.frame or an Arrow or DuckDB connection, is to use `class()` (or look at the environment tab, where Arrow and DuckDB connections will appear funky):

```{r check-object-type}

cat("R data.frame object:\n")
bef_head_df <- bef_ddb |> head() |>  dplyr::collect()
class(bef_head_df)
cat("\n")

cat("DuckDB connection object:\n")
bef_head_ddb <- bef_ddb |> head()
class(bef_head_ddb)
cat("\n")

cat("Arrow connection object:\n")
bef_head_arrow <- bef_folder |> arrow::open_dataset(unify_schemas = TRUE) |> head()
class(bef_head_arrow)
```

### 2: Filter

In this example, we only need adults alive and residing in Denmark between 2020 and 2024 for our analyses.

Note that we perform these filtering steps on the DuckDB connection object (not an R data.frame object) which allows `duckplyr` to inject it's magic into `dplyr`

```{r filter-bef}
bef_ddb_filtered <-
  bef_ddb |> dplyr::filter(year %in% 2020:2024 & alder >= 18) |>
  dplyr::select(pnr, year, foed_dag, koen, civst, reg, opr_land)
```

### 3: Convert to clean variables

Next, we'll generate the variables we want from the raw data to get a "clean" dataset. Again, note that we do the data processing on the DuckDB connection object (but to inspect/illustrate the changes, we'll load in some of the resulting dataset from DuckDB to R at the end of the chunk here):

```{r clean-bef}

bef_ddb_clean <- bef_ddb_filtered |>
  dplyr::mutate(
    pnr = pnr,
    year = year,
    date_of_birth = as.Date(foed_dag),
    
    sex = dplyr::if_else(koen == '1', 'Male', 'Female'),
    
    marital_status = dplyr::case_when(
      civst %in% c("G", "P") ~ "Married",
      civst %in% c("F", "O", "E", "L") ~ "Divorced or widowed",
      civst == "U" ~ "Unmarried",
      TRUE ~ NA_character_
    ),
    
    region = dplyr::case_when(
      reg == 81 ~ "North Denmark Region",
      reg == 82 ~ "Central Denmark Region",
      reg == 83 ~ "South Denmark Region",
      reg == 84 ~ "Capital Region",
      reg == 85 ~ "Zealand Region",
      TRUE ~ NA_character_
    ),
    
    immigrant = opr_land != 5100
  )

bef_ddb_clean |>  head() |> dplyr::collect()
```

## LPR

### LPR2

```{r setup-lpr2}

# Create lpr_adm directory/folder:
fs::dir_create(here::here("data-parquet", "lpr_adm"))

# Generate lpr_adm and save to disk
arrow::write_dataset(
  generate_lpr_adm(
    background_df = generate_background_pop()
    ),
  here::here("data-parquet", "lpr_adm"),
  format = "parquet",
  partitioning = "year")

# Create lpr_diag directory/folder:
fs::dir_create(here::here("data-parquet", "lpr_diag"))

# Generate lpr_adm and save to disk (year-partitioning the synthetic data is not feasible here)
arrow::write_dataset(
  generate_lpr_diag(generate_lpr_adm(
    background_df = generate_background_pop()
  )),
  here::here("data-parquet", "lpr_diag"),
  format = "parquet"
)
```

#### lpr_adm & lpr_diag

Let's look at `lpr_adm`:

```{r load-lpr-adm}
# Load lpr_adm data:

lpr_adm_folder <- here::here("data-parquet", "lpr_adm")
lpr_adm_ddb <- lpr_adm_folder |> arrow::open_dataset(unify_schemas = TRUE) |>
  # You can add cleaning steps in Arrow here before converting to DuckDB
  arrow::to_duckdb() |> dplyr::compute()

lpr_adm_df <- lpr_adm_ddb |> dplyr::collect()

head(lpr_adm_df)
```

And `lpr_diag:`

```{r load-lpr-diag}
# Load lpr_diag data:
lpr_diag_folder <- here::here("data-parquet", "lpr_diag")
lpr_diag_ddb <- lpr_diag_folder |> arrow::open_dataset(unify_schemas = TRUE) |>
  # You can add cleaning steps in Arrow here before converting to DuckDB
  arrow::to_duckdb() |> dplyr::compute()

lpr_diag_df <- lpr_diag_ddb |> dplyr::collect()

head(lpr_diag_df)
```

### LPR3

#### lpr_a_kontakt & lpr_a_diagnose

```{r setup-lpr-a}

# Create lpr_a_kontakt directory/folder:
fs::dir_create(here::here("data-parquet", "lpr_a_kontakt"))

# Generate lpr_a_kontakt and save to disk
arrow::write_dataset(
  generate_lpr_a_kontakt(
    background_df = generate_background_pop()
    ),
  here::here("data-parquet", "lpr_a_kontakt"),
  format = "parquet")

# Create lpr_a_diagnose directory/folder:
fs::dir_create(here::here("data-parquet", "lpr_a_diagnose"))

# Generate lpr_a_diagnose and save to disk
arrow::write_dataset(
  generate_lpr_a_diagnose(generate_lpr_a_kontakt(
    background_df = generate_background_pop()
    )),
  here::here("data-parquet", "lpr_a_diagnose"),
  format = "parquet"
)
```
